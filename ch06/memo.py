#RNNについて

#系列内の要素の出現がその前に現れた要素に依存するものであれば何でも良い
#the dog... -> barksみたいな？

#LSTMとGRUが多くの系列問題に役に立つ

#=====================================================================
#・SimpleRNNセル
#・RNNを用いたテキスト生成->alice.py
#・RNNのトポロジー
#・LSTM、GRU、およびその他のRNNの亜種
#=====================================================================

#SimpleRNN
#h_t = φ(h_(t-1), x_t)
#h_tとh_(t-1)は、それぞれの時間における隠れ状態の値
#この方程式は再帰的

#tanhなのは、2次微分の減衰が非常にゆっくりとゼロになることと関係があるため、勾配を活性化関数の線形領域に保持し、勾配消失問題に対処するのに役に立つ。
#時刻tの出力ベクトルy_tは重み行列Vと隠れ状態h_tの積にソフトマックス関数を適用したら得られる。
#h_t = tanh(Wh_(t-1) + U_x_t)
#y_t = softmax(Vh_t)

#-------------------------------------------------------------------
#・RNNのトポロジー
#-------------------------------------------------------------------

#勾配消失問題と勾配爆発問題
#RNNは遠く離れた依存性を学習しなくなってしまう。どんどん逆伝播していったら積なので小さくなる
#また、1よりも大きかったらどんどん膨れ上がってしまう。

#勾配消失問題解決策：行列Wを適切に初期化/tanhではなくReLU/教師なしで各層を事前学習
#最も一般的な解決策はLSTM or GRU

#LSTM：tanh層の代わりに4つのレイヤーが非常に特殊な方法で相互作用する。入力ゲートと忘却ゲートと出力ゲートを使う。
#勾配消失問題にかなり強い。

